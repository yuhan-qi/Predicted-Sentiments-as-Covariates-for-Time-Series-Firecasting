# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RTVrunN5-km6UEDFhlHW206q7XOl-KBL
"""

import pandas as pd
# import emoji
import nltk, re
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from collections import Counter

def clean_text(sent):
    # sent = sent.lower() 
    sent = sent.strip()
    sent = re.compile('<.*?>').sub('', sent)
    # sent = re.sub(emoji.get_emoji_regexp(),r"",sent)
    sent = re.sub('\s+', ' ', sent)
    return sent

wl = WordNetLemmatizer()

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    elif tag.startswith('S'):
        return wordnet.ADJ_SAT
    else:
        return wordnet.NOUN

def lemma_text(texts):
    final_text_list=[]
    for sent in texts:
        lemmatized_sentence = []
        sent = clean_text(sent)
        words = word_tokenize(sent)
        word_pos_tags = nltk.pos_tag(words)
        for idx, tag in enumerate(word_pos_tags):
            lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))
        lemmatized_text = " ".join(lemmatized_sentence)
        final_text_list.append(lemmatized_text)
    return final_text_list
